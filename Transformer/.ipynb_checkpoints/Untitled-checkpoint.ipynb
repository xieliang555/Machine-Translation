{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from jiwer import wer\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "            src_vocab_size: the source vocabulary size\n",
    "            tgt_vocab_size: the target vocabulary size\n",
    "            d_model: the embedding feature dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, src_vocab_size, tgt_vocab_size, d_model=512,\n",
    "                 nhead=8, num_enc_layers=6, num_dec_layers=6,\n",
    "                 dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_encoder = PostionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_enc_layers,\n",
    "                                          num_dec_layers, dim_feedforward,\n",
    "                                          dropout, activation)\n",
    "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def get_pad_mask(self, data):\n",
    "        # the index of '<pad>' is 1\n",
    "        mask = data.eq(1).transpose(0, 1)\n",
    "        mask = mask.masked_fill(mask == True, float(\n",
    "            '-inf')).masked_fill(mask == False, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def get_square_subsequent_mask(self, tgt):\n",
    "        seq_len = tgt.size(0)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(\n",
    "            0.0)).masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self.tgt_subsequent_mask = self.get_square_subsequent_mask(\n",
    "            tgt).to(self.device)\n",
    "        self.src_pad_mask = self.get_pad_mask(src).to(self.device)\n",
    "        self.tgt_pad_mask = self.get_pad_mask(tgt).to(self.device)\n",
    "        self.memory_pad_mask = self.get_pad_mask(src).to(self.device)\n",
    "\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.dropout(self.pos_encoder(src))\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.dropout(self.pos_encoder(tgt))\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=self.tgt_subsequent_mask,\n",
    "                               src_key_padding_mask=self.src_pad_mask,\n",
    "                               tgt_key_padding_mask=self.tgt_pad_mask,\n",
    "                               memory_key_padding_mask=self.memory_pad_mask)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PostionalEncoding(nn.Module):\n",
    "    \"\"\"docstring for PostionEncoder\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PostionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(- torch.arange(0, d_model,\n",
    "                                            2).float() * math.log(10000) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "def greedy_decoder(model, src, trg):\n",
    "    '''\n",
    "        only using src to generate decoder input\n",
    "    Args:\n",
    "        src: [S, N]\n",
    "        trg: [T, N]\n",
    "    Return:\n",
    "        trg: ['<sos>', 'w1', 'w2'...'wn']\n",
    "    '''\n",
    "    src_pad_mask = model.get_pad_mask(src)\n",
    "    tgt_pad_mask = model.get_pad_mask(tgt)\n",
    "    memory_pad_mask = model.get_pad_mask(src)\n",
    "    tgt_subsequent_mask = model.get_square_subsequence_mask(tgt)\n",
    "\n",
    "    encoder_outputs = model.transformer.encoder(\n",
    "        src, src_key_padding_mask=src_pad_mask)\n",
    "    decoder_inputs = copy.deepcopy(trg)\n",
    "    for t in range(1, len(decoder_inputs)):\n",
    "        decoder_outputs = model.transformer.decoder(\n",
    "            decoder_inputs, encoder_outputs, tgt_mask=tgt_subsequent_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=memory_pad_mask)\n",
    "        outs = model.out(decoder_outputs)\n",
    "        decoder_inputs[t] = outs.max(-1)[1][t-1]\n",
    "        \n",
    "    return decoder_inputs\n",
    "\n",
    "\n",
    "def train(model, train_iter, criterion, optimizer, TRG, epoch, writer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_bleu = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_bleu = 0.0\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:-1, :])\n",
    "        loss = criterion(\n",
    "            output.view(-1, output.shape[-1]), trg[1:, :].view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        batch_bleu = utils.count_bleu(output, trg[1:, :], TRG)\n",
    "        epoch_loss += batch_loss\n",
    "        epoch_bleu += batch_bleu\n",
    "        running_loss += batch_loss\n",
    "        running_bleu += batch_bleu\n",
    "\n",
    "        if batch_idx % 500 == 499:\n",
    "            writer.add_scalar('train loss',\n",
    "                              running_loss / 500,\n",
    "                              epoch * len(train_iter) + batch_idx)\n",
    "            writer.add_scalar('train bleu',\n",
    "                              running_bleu / 500,\n",
    "                              epoch * len(train_iter) + batch_idx)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_bleu = 0.0\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter, criterion, TRG, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_bleu = 0.0\n",
    "    for batch_idx, batch in enumerate(val_iter):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.trg.to(device)\n",
    "        output = model(src, tgt[:-1, :])\n",
    "        loss = criterion(\n",
    "            output.view(-1, output.shape[-1]), tgt[1:, :].view(-1))\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_bleu += utils.count_bleu(output, tgt[1:, :], TRG)\n",
    "\n",
    "    return epoch_loss / len(val_iter), epoch_bleu / len(val_iter)\n",
    "\n",
    "\n",
    "def test(model, test_iter, criterion, TRG, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_bleu = 0.0\n",
    "    for batch_idx, batch in enumerate(test_iter):\n",
    "        src = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        tgt = greedy_decoder(src, target[:-1, :])\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(\n",
    "            output.view(-1, output.shape[-1]), target[1:, :].view(-1))\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_bleu += utils.count_bleu(output, target[1:, :], TRG)\n",
    "\n",
    "    return epoch_loss / len(test_iter), epoch_bleu / len(test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
