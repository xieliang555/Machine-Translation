{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "\n",
    "    The code follows the architecture of \"NEURAL MACHINE TRANSLATION \n",
    "    BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\", which is Bidirectional\n",
    "    GRU encoder + unidirectional GRU decoder + Bahdanau attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time \n",
    "import os\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import NMT\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# field is the object for tokenize, padding and numericalize.\n",
    "# The arguments define the tokenizer, language, and padding information\n",
    "SRC = Field(tokenize='spacy',\n",
    "            tokenizer_language='de',\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize='spacy',\n",
    "            tokenizer_language='en',\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "\n",
    "train_data, dev_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                  fields=(SRC, TRG), root='.data', train='train',\n",
    "                                                  validation='val', test='test2016')\n",
    "\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "\n",
    "train_ietrator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, dev_data, test_data),\n",
    "    batch_size=2,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itos(idx_seq, field):\n",
    "    return [field.vocab.itos[idx] for idx in idx_seq]\n",
    "\n",
    "batch = next(iter(train_ietrator))\n",
    "src = batch.src.transpose(0,1)\n",
    "trg = batch.trg.transpose(0,1)\n",
    "src = [' '.join(itos(idx_seq, SRC)) for idx_seq in src]\n",
    "trg = [' '.join(itos(idx_seq, TRG)) for idx_seq in trg]\n",
    "# print(src)\n",
    "# print(trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "\n",
    "enc = NMT.Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "atten = NMT.Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "dec = NMT.Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, atten)\n",
    "model = NMT.Seq2Seq(enc, dec, device).to(device)\n",
    "model.apply(utils.init_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "writer = SummaryWriter(os.path.join('log/', time.strftime(\"%Y-%m-%d %H:%M:%S\" ,time.localtime(time.time()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_bleu = NMT.train(\n",
    "        model, criterion, train_ietrator, optimizer, CLIP, epoch, TRG, writer)\n",
    "    evaluate_loss, evaluate_bleu = NMT.evaluate(\n",
    "        model, criterion, valid_iterator, epoch, TRG, writer)\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_time_format = time.strftime(\"%H:%M:%S\", time.localtime(end_time))\n",
    "    epoch_mins, epoch_secs = utils.epoch_time(start_time, end_time)\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch+1:02} | Epoch duration: {epoch_mins}m {epoch_secs}s | Epoch end time: {end_time_format}')\n",
    "    print(f'\\tTrain loss: {train_loss:.3f} | Train bleu: {train_bleu:.3f}')\n",
    "    print(\n",
    "        f'\\tEvaluate loss: {evaluate_loss:.3f} | Evaluate bleu: {evaluate_bleu:.3f}\\n')\n",
    "\n",
    "\n",
    "test_loss, test_bleu = NMT.evaluate(\n",
    "    model, criterion, test_iterator, N_EPOCHS+1, TRG, writer)\n",
    "print(f'Test loss: {test_loss:.3f} | Test bleu: {test_bleu:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
