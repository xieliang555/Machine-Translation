{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description__:\n",
    "\n",
    "    The code follows the architecture of \"NEURAL MACHINE TRANSLATION \n",
    "    BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\", which is Bidirectional\n",
    "    GRU encoder + unidirectional GRU decoder + Bahdanau attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time \n",
    "import os\n",
    "from torchsummaryX import summary\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bleu(outputs, trg, TRG):\n",
    "    '''\n",
    "        outputs: [T, N, E]\n",
    "        trg: [T, N]\n",
    "    '''\n",
    "    outputs = outputs.permute(1,0,2).max(-1)[1]\n",
    "    trg = trg.permute(1,0)\n",
    "    mask = trg.ne(TRG.vocab.stoi['<pad>'])\n",
    "\n",
    "    outputs = outputs.masked_select(mask)\n",
    "    trg = trg.masked_select(mask)\n",
    "    \n",
    "    candidates = [[TRG.vocab.itos[i] for i in outputs]]\n",
    "    references = [[[TRG.vocab.itos[i] for i in trg]]]\n",
    "    return bleu_score(candidates, references)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    '''\n",
    "        initialize weights by normal distribution,\n",
    "        bias by constant distribution\n",
    "    '''\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSZ = 128\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SRC = Field(tokenize='spacy',\n",
    "            tokenizer_language='de',\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize='spacy',\n",
    "            tokenizer_language='en',\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "train_data, dev_data, test_data = Multi30k.splits(\n",
    "    exts=('.de', '.en'),\n",
    "    fields=(SRC, TRG), root='.data', train='train',\n",
    "    validation='val', test='test2016')\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, dev_data, test_data),\n",
    "    batch_size=BSZ, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# def itos(idx_seq, field):\n",
    "#     return [field.vocab.itos[idx] for idx in idx_seq]\n",
    "\n",
    "# batch = next(iter(train_ietrator))\n",
    "# src = batch.src.transpose(0,1)\n",
    "# trg = batch.trg.transpose(0,1)\n",
    "# src = [' '.join(itos(idx_seq, SRC)) for idx_seq in src]\n",
    "# trg = [' '.join(itos(idx_seq, TRG)) for idx_seq in trg]\n",
    "# print(src)\n",
    "# print(trg)\n",
    "\n",
    "print(len(train_iterator))\n",
    "print(len(valid_iterator))\n",
    "print(len(test_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 29\n",
    "CLIP = 1\n",
    "LR = 1e-3\n",
    "\n",
    "\n",
    "enc = NMT.Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "atten = NMT.Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "dec = NMT.Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, atten)\n",
    "model = NMT.Seq2Seq(enc, dec).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "path = f'bsz:{BSZ}-nepoch:{N_EPOCHS}-lr:{LR}-nemd:{ENC_EMB_DIM}-nhid:{ENC_EMB_DIM}\\\n",
    "-nattn:{ATTN_DIM}-dropout:{ENC_DROPOUT}-clip:{CLIP}'\n",
    "writer = SummaryWriter(os.path.join('./log', path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, iterator, optimizer, clip, epoch, TRG, writer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_bleu = 0\n",
    "    for batch_idx, batch in enumerate(iterator):\n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, trg, device)\n",
    "\n",
    "        loss = criterion(outputs[1:,...].view(-1, outputs.shape[-1]), trg[1:,:].view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_bleu += count_bleu(outputs[1:,...], trg[1:,:], TRG)\n",
    "        \n",
    "\n",
    "        if batch_idx % 27 == 26:\n",
    "            writer.add_scalar('train loss',\n",
    "                              running_loss/27,\n",
    "                              epoch*len(iterator)+batch_idx)\n",
    "\n",
    "            writer.add_scalar('train BLEU',\n",
    "                              running_bleu/27,\n",
    "                              epoch*len(iterator)+batch_idx)\n",
    "\n",
    "            running_bleu = 0\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, iterator, epoch, TRG, writer, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_bleu = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(iterator):\n",
    "            src = batch.src.to(device)\n",
    "            trg = batch.trg.to(device)\n",
    "            # set teacher forcing ratio = 0 \n",
    "            outputs = model(src, trg, device, 0)\n",
    "\n",
    "            loss = criterion(outputs[1:,...].view(-1, outputs.shape[-1]), trg[1:,:].view(-1))\n",
    "            running_loss += loss.item()\n",
    "            running_bleu += count_bleu(outputs[1:,...], trg[1:,:], TRG)\n",
    "\n",
    "            if batch_idx % 1 == 0:\n",
    "                writer.add_scalar('test loss',\n",
    "                                  running_loss/1,\n",
    "                                  epoch*len(iterator)+batch_idx)\n",
    "\n",
    "                writer.add_scalar('test BLEU',\n",
    "                                  running_bleu/1,\n",
    "                                  epoch*len(iterator)+batch_idx)\n",
    "\n",
    "                running_loss = 0\n",
    "                running_bleu = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 1min, total: 2min 33s\n",
      "Wall time: 58.2 s\n",
      "CPU times: user 2.76 s, sys: 1.14 s, total: 3.9 s\n",
      "Wall time: 1.29 s\n",
      "0\n",
      "CPU times: user 1min 31s, sys: 1min 3s, total: 2min 35s\n",
      "Wall time: 57 s\n",
      "CPU times: user 2.38 s, sys: 1.35 s, total: 3.73 s\n",
      "Wall time: 1.12 s\n",
      "1\n",
      "CPU times: user 1min 36s, sys: 56.5 s, total: 2min 32s\n",
      "Wall time: 59.2 s\n",
      "CPU times: user 2.64 s, sys: 1.17 s, total: 3.81 s\n",
      "Wall time: 1.13 s\n",
      "2\n",
      "CPU times: user 1min 35s, sys: 59.4 s, total: 2min 35s\n",
      "Wall time: 1min\n",
      "CPU times: user 2.15 s, sys: 1.64 s, total: 3.78 s\n",
      "Wall time: 1.05 s\n",
      "3\n",
      "CPU times: user 1min 34s, sys: 1min 2s, total: 2min 37s\n",
      "Wall time: 1min\n",
      "CPU times: user 2.08 s, sys: 1.7 s, total: 3.78 s\n",
      "Wall time: 1.13 s\n",
      "4\n",
      "CPU times: user 1min 36s, sys: 1min, total: 2min 37s\n",
      "Wall time: 1min 1s\n",
      "CPU times: user 2.61 s, sys: 1.2 s, total: 3.81 s\n",
      "Wall time: 1.21 s\n",
      "5\n",
      "CPU times: user 1min 38s, sys: 1min 1s, total: 2min 39s\n",
      "Wall time: 1min 3s\n",
      "CPU times: user 2.53 s, sys: 1.26 s, total: 3.79 s\n",
      "Wall time: 1.13 s\n",
      "6\n",
      "CPU times: user 1min 39s, sys: 59.8 s, total: 2min 39s\n",
      "Wall time: 1min 4s\n",
      "CPU times: user 2.28 s, sys: 1.62 s, total: 3.9 s\n",
      "Wall time: 1.24 s\n",
      "7\n",
      "CPU times: user 1min 40s, sys: 1min, total: 2min 40s\n",
      "Wall time: 1min 4s\n",
      "CPU times: user 2.78 s, sys: 1.23 s, total: 4.01 s\n",
      "Wall time: 1.36 s\n",
      "8\n",
      "CPU times: user 1min 40s, sys: 1min 1s, total: 2min 41s\n",
      "Wall time: 1min 7s\n",
      "CPU times: user 2.56 s, sys: 1.33 s, total: 3.89 s\n",
      "Wall time: 1.24 s\n",
      "9\n",
      "CPU times: user 1min 42s, sys: 58.4 s, total: 2min 40s\n",
      "Wall time: 1min 5s\n",
      "CPU times: user 2.71 s, sys: 1.32 s, total: 4.03 s\n",
      "Wall time: 1.46 s\n",
      "10\n",
      "CPU times: user 1min 45s, sys: 57.1 s, total: 2min 42s\n",
      "Wall time: 1min 8s\n",
      "CPU times: user 2.55 s, sys: 1.48 s, total: 4.03 s\n",
      "Wall time: 1.35 s\n",
      "11\n",
      "CPU times: user 1min 44s, sys: 58.6 s, total: 2min 43s\n",
      "Wall time: 1min 10s\n",
      "CPU times: user 2.83 s, sys: 1.21 s, total: 4.04 s\n",
      "Wall time: 1.46 s\n",
      "12\n",
      "CPU times: user 1min 45s, sys: 59 s, total: 2min 44s\n",
      "Wall time: 1min 11s\n",
      "CPU times: user 2.52 s, sys: 1.37 s, total: 3.89 s\n",
      "Wall time: 1.38 s\n",
      "13\n",
      "CPU times: user 1min 46s, sys: 55 s, total: 2min 41s\n",
      "Wall time: 1min 9s\n",
      "CPU times: user 2.77 s, sys: 1.32 s, total: 4.1 s\n",
      "Wall time: 1.56 s\n",
      "14\n",
      "CPU times: user 1min 47s, sys: 58.2 s, total: 2min 45s\n",
      "Wall time: 1min 12s\n",
      "CPU times: user 2.8 s, sys: 1.26 s, total: 4.06 s\n",
      "Wall time: 1.52 s\n",
      "15\n",
      "CPU times: user 1min 46s, sys: 59.6 s, total: 2min 46s\n",
      "Wall time: 1min 13s\n",
      "CPU times: user 2.74 s, sys: 1.26 s, total: 4 s\n",
      "Wall time: 1.53 s\n",
      "16\n",
      "CPU times: user 1min 51s, sys: 57.5 s, total: 2min 48s\n",
      "Wall time: 1min 16s\n",
      "CPU times: user 2.61 s, sys: 1.45 s, total: 4.06 s\n",
      "Wall time: 1.52 s\n",
      "17\n",
      "CPU times: user 1min 50s, sys: 58.8 s, total: 2min 49s\n",
      "Wall time: 1min 15s\n",
      "CPU times: user 2.87 s, sys: 1.33 s, total: 4.2 s\n",
      "Wall time: 1.64 s\n",
      "18\n",
      "CPU times: user 1min 48s, sys: 1min 2s, total: 2min 51s\n",
      "Wall time: 1min 14s\n",
      "CPU times: user 3.01 s, sys: 1.44 s, total: 4.44 s\n",
      "Wall time: 1.76 s\n",
      "19\n",
      "CPU times: user 1min 48s, sys: 59.7 s, total: 2min 48s\n",
      "Wall time: 1min 13s\n",
      "CPU times: user 2.97 s, sys: 1.38 s, total: 4.35 s\n",
      "Wall time: 1.58 s\n",
      "20\n",
      "CPU times: user 1min 48s, sys: 1min 2s, total: 2min 50s\n",
      "Wall time: 1min 12s\n",
      "CPU times: user 2.93 s, sys: 1.4 s, total: 4.33 s\n",
      "Wall time: 1.66 s\n",
      "21\n",
      "CPU times: user 1min 48s, sys: 1min, total: 2min 49s\n",
      "Wall time: 1min 12s\n",
      "CPU times: user 3.08 s, sys: 1.23 s, total: 4.31 s\n",
      "Wall time: 1.73 s\n",
      "22\n",
      "CPU times: user 1min 49s, sys: 58.4 s, total: 2min 47s\n",
      "Wall time: 1min 12s\n",
      "CPU times: user 2.69 s, sys: 1.52 s, total: 4.22 s\n",
      "Wall time: 1.62 s\n",
      "23\n",
      "CPU times: user 1min 50s, sys: 1min 1s, total: 2min 52s\n",
      "Wall time: 1min 15s\n",
      "CPU times: user 3.45 s, sys: 1.07 s, total: 4.52 s\n",
      "Wall time: 1.78 s\n",
      "24\n",
      "CPU times: user 1min 50s, sys: 1min, total: 2min 50s\n",
      "Wall time: 1min 14s\n",
      "CPU times: user 3.06 s, sys: 1.4 s, total: 4.47 s\n",
      "Wall time: 1.75 s\n",
      "25\n",
      "CPU times: user 1min 51s, sys: 56.8 s, total: 2min 47s\n",
      "Wall time: 1min 14s\n",
      "CPU times: user 2.93 s, sys: 1.62 s, total: 4.55 s\n",
      "Wall time: 1.91 s\n",
      "26\n",
      "CPU times: user 1min 51s, sys: 56.6 s, total: 2min 48s\n",
      "Wall time: 1min 16s\n",
      "CPU times: user 3.06 s, sys: 1.38 s, total: 4.44 s\n",
      "Wall time: 1.78 s\n",
      "27\n",
      "CPU times: user 1min 52s, sys: 58.7 s, total: 2min 50s\n",
      "Wall time: 1min 15s\n",
      "CPU times: user 2.99 s, sys: 1.44 s, total: 4.43 s\n",
      "Wall time: 1.65 s\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    %time train(model, criterion, train_iterator, optimizer, CLIP, epoch, TRG, writer, device)\n",
    "    %time evaluate(model, criterion, valid_iterator, epoch, TRG, writer, device)\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
